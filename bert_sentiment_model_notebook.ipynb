{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project, I have perfomed: Fine-tuning BERT for text classification — the foundation for all modern NLP pipelines. I have used Hugging Face Transformers with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Required Dependencies\n",
    "# -----------------------\n",
    "!pip install transformers\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:51:33.729201Z",
     "iopub.status.busy": "2025-11-17T04:51:33.728645Z",
     "iopub.status.idle": "2025-11-17T04:51:57.944686Z",
     "shell.execute_reply": "2025-11-17T04:51:57.944066Z",
     "shell.execute_reply.started": "2025-11-17T04:51:33.729176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 04:51:44.904757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763355105.103125     125 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763355105.156957     125 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Required Imports \n",
    "# -----------------------\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification # This 'transformers' s a hugging face library. 'BertForSequenceClassification' comes from Pytorch.\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:52:05.681136Z",
     "iopub.status.busy": "2025-11-17T04:52:05.680311Z",
     "iopub.status.idle": "2025-11-17T04:52:09.631308Z",
     "shell.execute_reply": "2025-11-17T04:52:09.630680Z",
     "shell.execute_reply.started": "2025-11-17T04:52:05.681113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daedc49fd7549eda879d96378039c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28771f3a80b4458784558339839a7f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fefd7a4733e45a085d84c65d06ba745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d083f74aaa4dcdb9f896228311548f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e685280ab746ac97cc8fa9dc05ad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b3e776dbc5416f9670e72a4a384254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728bc14f9ae0484d8cef86a3a388fd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\") # Loading IMDB dataset for sentiment analysis.\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Cleaning the dataset \n",
    "# -----------------------\n",
    "#  There are lot of HTML tags in the text (reviews). Note, the BertTokenizer automatically handles and removes the rare 'symbols', preserving only the common ones.\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text) # Removes all the HTML tags since there are many.\n",
    "    text = re.sub(r'\\s+', ' ', text) # Removes extra spaces\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Applying mapping the current text and saving the clean version in the same dictionary keys (train & test)\n",
    "dataset = dataset.map(lambda x: {'text': clean_text(x['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:52:16.507890Z",
     "iopub.status.busy": "2025-11-17T04:52:16.507318Z",
     "iopub.status.idle": "2025-11-17T04:52:17.146975Z",
     "shell.execute_reply": "2025-11-17T04:52:17.146276Z",
     "shell.execute_reply.started": "2025-11-17T04:52:16.507867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Label: 12500\n",
      "Positive Label: 12500\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Checking if dataset is balanced or not\n",
    "# ---------------------------------------------\n",
    "labels = dataset[\"train\"][\"label\"]\n",
    "neg_label = labels.count(0)\n",
    "pos_label = labels.count(1)\n",
    "\n",
    "print(f\"Negative Label: {neg_label}\")\n",
    "print(f\"Positive Label: {pos_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Tokenization\n",
    "# --------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # bert-base-uncased → standard English BERT. Tokenizer converts words → token IDs. The 'BertTokenizer' is also a pretrained model just like the 'BERT' itself (Here, 'bert-base-uncased' is a pretrained-english-tokenizer).\n",
    "# Performing Tokenization in batches which is recommended for faster computing\n",
    "# Also, the 'BertTokenizer' is already lower-cased. It also pads the sequences.\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'],  padding = 'max_length', max_length = 128, truncation = True) # By default, the padding is 'post' that's why not written padding = 'post', instead written padding = 'max_len' which will pad each sequence to the maximum length which 128. 128 is generally sufficient for small texts(reviews, emails, etc). truncate = True will truncate sentences longer than 128 length.\n",
    "\n",
    "dataset = dataset.map(tokenize, batched = True)\n",
    "\n",
    "print(dataset)  # The tokenizer returns a dictionary with multiple keys, like: 'input_ids' → token IDs for each word/sub word, 'token_type_ids' → segment IDs (used by BERT for sentence pairs), 'attention_mask' → 1 for real tokens, 0 for padding. map() automatically adds these new columns to your dataset while keeping the original ones ('text' and 'label').\n",
    "# Also, note that all the features in this dataset-dictionary (even 'text'-feature) are used by the model internally for validation and loss calculation. \"But, how 'text' will be used for loss_calculation and validation?\" - The model will compute the 'token_ids' for the actual input sentences each time when it wants in order to calculate the loss and compare it with the predicted 'input_ids'.\n",
    "\n",
    "\n",
    "dataset.set_format(type = 'torch', columns = ['input_ids', 'attention_mask', 'label']) # Keep only the necessary columns for model training. 'torch' is the specifier to make the dataset as 'torch' tensors.\n",
    "\n",
    "# Splitting the test data into test and validation data\n",
    "dataset = load_from_disk(\"/kaggle/input/train-test-validation-tokenizer-of-imdb-dataset/IMDB_train_test_validation_tokenizer\") ## Optional loading.\n",
    "test_split = dataset[\"test\"].train_test_split(test_size = 0.2, seed = 42)\n",
    "test_dataset = test_split[\"train\"]\n",
    "valid_dataset = test_split[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size = 16, shuffle = True) # Creating DataLoaders.\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 16) # Validation dataset is required for temperature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:52:28.961247Z",
     "iopub.status.busy": "2025-11-17T04:52:28.960457Z",
     "iopub.status.idle": "2025-11-17T04:52:29.373894Z",
     "shell.execute_reply": "2025-11-17T04:52:29.373261Z",
     "shell.execute_reply.started": "2025-11-17T04:52:28.961195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # This block is executable when tokenized dataset is to be loaded from disk.\n",
    "# # Splitting the test data into test and validation data\n",
    "# dataset = load_from_disk(\"/kaggle/input/train-test-validation-tokenizer-of-imdb-dataset/IMDB_train_test_validation_tokenizer\") ## Optional loading.\n",
    "# test_split = dataset[\"test\"].train_test_split(test_size = 0.2, seed = 42, load_from_cache_file=False, test_indices_cache_file_name=\"/kaggle/working/test.idx\",train_indices_cache_file_name=\"/kaggle/working/valid.idx\")\n",
    "\n",
    "# test_dataset = test_split[\"train\"]\n",
    "# valid_dataset = test_split[\"test\"]\n",
    "\n",
    "# train_loader = DataLoader(dataset['train'], batch_size = 16, shuffle = True) # Creating DataLoaders.\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 16)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size = 16) # Validation dataset is required for temperature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"IMDB_train_test_validation_tokenizer\")\n",
    "!zip -r IMDB_train_test_validation_tokenizer.zip IMDB_train_test_validation_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:52:56.171168Z",
     "iopub.status.busy": "2025-11-17T04:52:56.170590Z",
     "iopub.status.idle": "2025-11-17T04:58:14.053245Z",
     "shell.execute_reply": "2025-11-17T04:58:14.052530Z",
     "shell.execute_reply.started": "2025-11-17T04:52:56.171142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc7b64da32546318e5e002dfab5d729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf248919124a43e98bb37d376e93f98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.3485\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# Model Training \n",
    "# -----------------\n",
    "# The workflow and methods to train and compile is different from Tensorflow in Pytorch\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2) # bert-base-uncased is: A pretrained English-language BERT model (base size, lowercase text) trained on the BooksCorpus and English Wikipedia datasets\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5) # Using the AdamW optimizer because: AdamW --> Adam + correct weight decay → better generalization, stable training, less overfitting. Use the AdamW from the hugging face itself. This much Learning rate is a sweet-spot for BERT and is recommended.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --------- Training Loop ---------\n",
    "model.train() # This: Puts the model in training mode and Activates dropout and batch normalization (training-specific layers). Without 'model.train()' these layers will behave as in evaluation mode, which will make the training inaccurate.\n",
    "for epoch in range(1): # Training with only 1 epoch is sufficient for checking if whether everything is working fine or not.\n",
    "    total_loss = 0 # This will capture the loss at each batch.\n",
    "    for batch in train_loader: # Iterating through each batch.\n",
    "        optimizer.zero_grad() # This will reset the gradients of the previous batch for the new batch.\n",
    "\n",
    "        outputs = model(**{k: v.to(device) for k, v in batch.items() if k != 'label'}, labels = batch['label'].to(device)) # outputs is a SequenceClassifierOutput object.\n",
    "        \n",
    "        loss = outputs.loss # outputs is a SequenceClassifierOutput object with: outputs.logits → the raw predictions and outputs.loss → the loss computed automatically\n",
    "        total_loss += loss.item() # loss.item() → converts the PyTorch tensor to a Python float so we can accumulate it. total_loss += loss.item() → sum the losses across all batches.\n",
    "\n",
    "        loss.backward() # This is a Back propagation which calculates gradients for all model parameters.\n",
    "        optimizer.step() # This Updates the model weights based on the gradients.\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1} | Training Loss: {total_loss / len(train_loader):.4f}\") # len(train_loader) will give the total no. of batches. Dividing it with the accumulated total_loss gives an average loss of each batch.\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:58:14.054590Z",
     "iopub.status.busy": "2025-11-17T04:58:14.054377Z",
     "iopub.status.idle": "2025-11-17T04:58:31.621537Z",
     "shell.execute_reply": "2025-11-17T04:58:31.620907Z",
     "shell.execute_reply.started": "2025-11-17T04:58:14.054572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Temperature Scaling\n",
    "# -------------------------\n",
    "# Taking the output logits of validation dataset for teaching the temperature and later, I will use test ataset for evaluation on scaled logits. We need to teach the temperature on the validation data and then use the temperature to scale the logits for test data.\n",
    "model.eval() # Puts the model into evaluation mode. Disables dropout and other training-only layers for consistent results.\n",
    "\n",
    "# Creating the list of labels of the entire test dataset + the list of all the probabilites outputs.\n",
    "labels_list = [] # It stores all the labels\n",
    "logits_list = []\n",
    "\n",
    "with torch.no_grad(): # torch.no_grad() → disables gradient calculation (saves memory and speeds up evaluation).\n",
    "    for batch in valid_loader:\n",
    "        outputs = model(**{k: v.to(device) for k, v in batch.items() if k != 'label'})\n",
    "        logits_list.append(outputs.logits.cpu()) #'cpu' is needed for numpy during ECE calculation.\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "labels_ts = torch.cat(labels_list)\n",
    "logits_ts = torch.cat(logits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:58:31.622528Z",
     "iopub.status.busy": "2025-11-17T04:58:31.622270Z",
     "iopub.status.idle": "2025-11-17T04:58:31.703056Z",
     "shell.execute_reply": "2025-11-17T04:58:31.702366Z",
     "shell.execute_reply.started": "2025-11-17T04:58:31.622504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Temperature: 0.8900\n"
     ]
    }
   ],
   "source": [
    "# Optimizing Temperature\n",
    "# The aim is to get the actual and correct probability values of test data on the basis of scaled logits. Since we have got the value of ECE, it's the time to get the real probability values of test data and then compute the ECE again on those probability values to get the actual ECE value for correct inference. To do this; we will perform temperature scaling to scale the output logits.\n",
    "temperature = torch.tensor(1.0, dtype = torch.float32, requires_grad = True) \n",
    "optimizer_ts = torch.optim.LBFGS([temperature], lr = 0.01, max_iter = 50) # Updates temperature value after each iteration inside eval_fn.\n",
    "\n",
    "criterion_ts = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def eval_fn():\n",
    "    optimizer_ts.zero_grad()\n",
    "    scaled_logits = logits_ts / temperature # Scaled Logits\n",
    "    loss = criterion_ts(scaled_logits, labels_ts)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer_ts.step(eval_fn) # Runs the eval_fn repeatdly until max_iter.\n",
    "print(f\"Learned Temperature: {temperature.item():.4f}\") # 'temperature' will be automatically updated. Now, this temperature has been successfully learnt throught the outputs.logits, now this temperature will be used to scale the same outputs.logits to calculate the real probabilities and then to calculate to real ECE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:58:31.705194Z",
     "iopub.status.busy": "2025-11-17T04:58:31.704973Z",
     "iopub.status.idle": "2025-11-17T04:58:31.708964Z",
     "shell.execute_reply": "2025-11-17T04:58:31.708384Z",
     "shell.execute_reply.started": "2025-11-17T04:58:31.705176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Temperature function\n",
    "# ----------------------\n",
    "def apply_temperature(logits):\n",
    "    return logits / temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:58:31.709918Z",
     "iopub.status.busy": "2025-11-17T04:58:31.709660Z",
     "iopub.status.idle": "2025-11-17T04:59:40.065085Z",
     "shell.execute_reply": "2025-11-17T04:59:40.064264Z",
     "shell.execute_reply.started": "2025-11-17T04:58:31.709903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.3000%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Evaluation and Probability calculation loop\n",
    "# -------------------------------------------------\n",
    "total, correct = 0.0 ,0.0 # correct → counts correct predictions. total → counts total samples.\n",
    "model.eval()\n",
    "probs1 = []\n",
    "labels1 = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(**{k: v.to(device) for k, v in batch.items() if k!='label'})\n",
    "        labels = batch['label'].to(device)\n",
    "        labels1.append(labels.cpu()) # Numpy expects cpu tensors.\n",
    "        scaled_logits = apply_temperature(outputs.logits) # Now, I am using scaled_logits as output logits to get softmax probabilities.\n",
    "        probs = torch.softmax(scaled_logits, dim = 1)[:, 1]\n",
    "        probs1.append(probs.cpu())\n",
    "        preds = torch.argmax(scaled_logits, dim = 1) # torch.argmax(..., dim=1) → takes the index of the highest logit, i.e., predicted class (0 or 1). Basically, 'outputs.logits' is are the values that the softmax takes to make them between 0-1. We can use the same logic of 'torch.argamx' because, we get the same output label if we use '.argmax(softmax)' or '.argmax(outputs.logits)'. Note: 'outputs.logits' not the probability values.\n",
    "        correct += (preds == labels).sum().item() # 'This compares predicted labels with true labels element-wise. .sum() Counts how many True values are there. In PyTorch, True is treated as 1 and False as 0. .item() Converts a single-element tensor to a Python number, why? because PyTorch tensors are not plain numbers, and for arithmetic or printing, we usually want a float/int.\n",
    "        total += labels.size(0) # batch['label'] - This is a tensor containing the true labels for the current batch. '.size(0)' will give the total no. of values in each batch and give the final total value as a value which is the sum of all the values in all batches.\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.4f}%\") # Total samples / Correct samples = Test accuracy. Multiplying with 100 to get the accuracy in percentage.        \n",
    "probs1_numpy = torch.cat(probs1).numpy() # '.cat' concatenates all the list of pytorch-tensors into a single pytorch-tensor, then numpy converts them into numpy array.\n",
    "labels1_numpy = torch.cat(labels1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:40.066354Z",
     "iopub.status.busy": "2025-11-17T04:59:40.066063Z",
     "iopub.status.idle": "2025-11-17T04:59:40.078855Z",
     "shell.execute_reply": "2025-11-17T04:59:40.078140Z",
     "shell.execute_reply.started": "2025-11-17T04:59:40.066332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE: 0.3757\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Expected Calibration Error (ECE)\n",
    "# ---------------------------------------\n",
    "def expected_calibration_error(probs, labels, n_bins = 15):\n",
    "    bins = np.linspace(0, 1, n_bins + 1) # Gives 16 values between 0 and 1 separated by equal space. E.g., (0, 0.2, 0.4,..)\n",
    "    ece = 0.0 # Stores the Expected Calibration Error\n",
    " \n",
    "    for i in range(n_bins):\n",
    "        bin_lower, bin_upper = bins[i], bins[i+1]\n",
    "        in_bin = (probs > bin_lower) & (probs <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin) # This gives proportion of probs lying in the bin.\n",
    "        if prop_in_bin > 0:\n",
    "            acc = np.mean(labels[in_bin] == (probs[in_bin] > 0.5)) # Calculating accuracy.\n",
    "            conf = np.mean(probs[in_bin])\n",
    "            ece += np.abs(acc - conf) * prop_in_bin\n",
    "    return ece\n",
    "\n",
    "ece = expected_calibration_error(probs1_numpy, labels1_numpy)\n",
    "print(f\"ECE: {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before TEMPERATURE SCALING, ECE score was 0.4186 and Test Accuracy was 85.0150 - The model's accurate 85% of the time; but the confidence of the model is higher from the accuracy which means the model is overconfident and we can't trust its predictions.\n",
    "\n",
    "# After TEMPERATURE SCALING, ECE SCORE is 0.3986 and Test Accuracy is 88.4500% - There is a 2 percent increase in the ECE even after temperature scaling because I have trained the model for 1 epoch only due to resource shortage. Yet accuracy is a little higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:40.079750Z",
     "iopub.status.busy": "2025-11-17T04:59:40.079508Z",
     "iopub.status.idle": "2025-11-17T04:59:40.093526Z",
     "shell.execute_reply": "2025-11-17T04:59:40.092787Z",
     "shell.execute_reply.started": "2025-11-17T04:59:40.079730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brier Score: 0.0847'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Brier Score Computation\n",
    "# ------------------------------------------\n",
    "def compute_brier_score(probs, labels):\n",
    "    brier_score = np.mean((probs - labels) ** 2)\n",
    "    return f\"Brier Score: {brier_score:.4f}\"\n",
    "\n",
    "compute_brier_score(probs1_numpy, labels1_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier Score is 0.0847. This means the model's probabilities are closer to the true labels, but the model is not reliable because it is overconfident as observed in the ECE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:40.094477Z",
     "iopub.status.busy": "2025-11-17T04:59:40.094309Z",
     "iopub.status.idle": "2025-11-17T04:59:40.835242Z",
     "shell.execute_reply": "2025-11-17T04:59:40.834590Z",
     "shell.execute_reply.started": "2025-11-17T04:59:40.094464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acccb07f60e40d28bc7023745b8ca2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65ec21046ea4772a19edefd1deab6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770eddd09c8a4c3bb652324fdcbbecf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Testing \n",
    "# ----------------------\n",
    "sample_texts = [\"I loved this movie!\", \"This movie was terrible.\"]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(sample_texts, max_length = 128, padding = 'max_length', truncation = True, return_tensors=\"pt\") # This will return the output as a tensorflow tensors.\n",
    "outputs = model(**{k: v.to(device) for k, v in inputs.items()})\n",
    "\n",
    "predictions = torch.argmax(outputs.logits, dim = 1) # 'outputs.logits' will return the shape of (batch_size, num_labels — 0&1 here). Since the labels are at the axis 1 so, I am using that axis for fetching the max value.\n",
    "print(predictions.cpu().numpy()) # Numpy expects a CPU computation only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model has predicted the sample texts correctly even if it was trained with only 1 epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:40.836305Z",
     "iopub.status.busy": "2025-11-17T04:59:40.836014Z",
     "iopub.status.idle": "2025-11-17T04:59:42.433065Z",
     "shell.execute_reply": "2025-11-17T04:59:42.432434Z",
     "shell.execute_reply.started": "2025-11-17T04:59:40.836287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.save_pretrained(\"./bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:42.434990Z",
     "iopub.status.busy": "2025-11-17T04:59:42.434776Z",
     "iopub.status.idle": "2025-11-17T04:59:43.654568Z",
     "shell.execute_reply": "2025-11-17T04:59:43.653952Z",
     "shell.execute_reply.started": "2025-11-17T04:59:42.434974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/bert_sentiment_model/tokenizer_config.json',\n",
       " '/kaggle/working/bert_sentiment_model/special_tokens_map.json',\n",
       " '/kaggle/working/bert_sentiment_model/vocab.txt',\n",
       " '/kaggle/working/bert_sentiment_model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/kaggle/working/bert_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:43.655635Z",
     "iopub.status.busy": "2025-11-17T04:59:43.655326Z",
     "iopub.status.idle": "2025-11-17T04:59:43.671728Z",
     "shell.execute_reply": "2025-11-17T04:59:43.671184Z",
     "shell.execute_reply.started": "2025-11-17T04:59:43.655617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./kaggle/working/bert_sentiment_mode/tokenizer_config.json',\n",
       " './kaggle/working/bert_sentiment_mode/special_tokens_map.json',\n",
       " './kaggle/working/bert_sentiment_mode/vocab.txt',\n",
       " './kaggle/working/bert_sentiment_mode/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./kaggle/working/bert_sentiment_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:59:43.673229Z",
     "iopub.status.busy": "2025-11-17T04:59:43.672491Z",
     "iopub.status.idle": "2025-11-17T05:00:06.286179Z",
     "shell.execute_reply": "2025-11-17T05:00:06.285085Z",
     "shell.execute_reply.started": "2025-11-17T04:59:43.673195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: bert_sentiment_model/ (stored 0%)\n",
      "  adding: bert_sentiment_model/model.safetensors (deflated 7%)\n",
      "  adding: bert_sentiment_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_sentiment_model/vocab.txt (deflated 53%)\n",
      "  adding: bert_sentiment_model/config.json (deflated 49%)\n",
      "  adding: bert_sentiment_model/special_tokens_map.json (deflated 42%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r bert_sentiment_model.zip bert_sentiment_model\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8751064,
     "sourceId": 13752729,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
